{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital == Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to really get used to when dealing with digital images is the concept of quantization. Because we have to represent images in a finite fashion, when real objects and surfaces are continuous surfaces, this requires us to simplify the information. \n",
    "\n",
    "Let's illustrate this with a function, before we talk about images. Say we have the function $y = 2 x^2$. The continuous representation would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, the function as I've defined it *isn't* continuous. In this case, I have taken 101 *samples* of x and y to represent the function (that's what the `np.linspace` function does). It would be more accurate to represent the data thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be easier to visualize if I reduce the sampling even more to say, 21 points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If an image is represented as a continuous 2D function $f(x,y)$, where the output is the pixel value at the coordinates $x,y$, we need to sample in both coordinates and amplitude. Digitizing coordinate values is called **sampling**. Digitizing amplitude values is called **quantization**.\n",
    "\n",
    "After performing this digitizing process, an image becomes a matrix of pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zooming and Shrinking Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zooming into an image to make it bigger can be thought of as *oversampling* an image. It requires two steps: \n",
    "\n",
    "1. creating new pixel locations\n",
    "2. assigning gray levels to these new locations\n",
    "\n",
    "If we have a 500x500 pixel image, and we want to enlarge it 1.5 times to get a 750x750 image, you could visualize this process by imposing a 750x750 grid on the existing image. Pixel levels are assigned by *nearest neighbor interpolation*. Nearest-neighbor interpolation is fast, but creates a checkerboard effect that is undesirable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get rid of some of this blockiness from zooming by using a form of interpolation using the four nearest neighbours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially the same process is done when shrinking images. You have to re-sample at a lower rate, and you can do different forms of interpolation to fill in the pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is generally a good idea to blur the image first before rescaling the image. This gets rid of an effect known as *aliasing*. Aliasing has to do with high-frequency information in a signal or image. Blurring the image first removes high-frequency information before down-sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixel Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pixel p has four *horizontal* and *vertical* neighbors, with coordinates given by:\n",
    "\n",
    "$$(x+1,y), (x-1,y), (x,y+1), (x,y-1) $$\n",
    "\n",
    "This set of pixels is called the *4-neighbors* of p, $N_4(p)$. The four *diagonal* neighbors of p have coordinates:\n",
    "\n",
    "$$(x+1,y+1),(x+1,y-1), (x-1,y+1), (x-1,y-1) $$\n",
    "\n",
    "These, together with the 4-neighbors of p, comprise the *8-neighbors* of p, $N_8(p)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjacency, Connectivity, Regions, Boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let V be the set of gray values used to define adjacency. In a binary image, $V=\\{1\\}$ if we are referring to adjacent pixels with a value of 1.\n",
    "\n",
    "1. *4-adjacency*: Two pixels p and q with values from V are 4-adjacent if q is in the set $N_4(p)$.\n",
    "2. *8-adjacency*: Two pixels p and q with values from V are 8-adjacent if q is in the set $N_8(p)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *path* or *curve* from pixel p at (x,y) to pixel q at (s,t) is a sequence of distinct pixels with coordinates\n",
    "\n",
    "$$(x_0, y_0), (x_1, y_1), (x_n, y_n) $$\n",
    "\n",
    "where n is the *length* of the path.\n",
    "\n",
    "Let S represent a subset of pixels in an image. Two pixels p and q are said to be *connected* in S i there exists a path between them consisting entirely of pixels in S.\n",
    "\n",
    "Let R represent a subset of pixels in an image. R is a *region* of the image if R is a *connected set*. The *boundary* (or *border* or *contour*) of R is the set of pixels in the region that have one or more neighbors that are not in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pixels p, q, and z with coordinates (x,y), (s,t), and (v,w), respectively, D is a *distance function* if \n",
    "\n",
    "1. $D(p,q) \\ge 0$\n",
    "2. $D(p,q) = D(q,p)$\n",
    "3. $D(p,z) \\le D(p,q) + D(q,z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Euclidian distance* is defined as\n",
    "\n",
    "$$D_e(p,q) = [(x-s)^2 + (y-t)^2]^{1/2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *D4 distance* or *city-block distance* between p and q is defined as:\n",
    "\n",
    "$$D_4(p,q) = \\mid x - s \\mid + \\mid y - t \\mid $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *D8 distance* or *chessboard distance* between p and q is defined as:\n",
    "\n",
    "$$D_8(p,q) = \\max{(\\mid x - s \\mid, \\mid y - t \\mid)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Class Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try implementing a few other distance metrics from the `scipy.spatial.distance` package. As these functions only calculate the distance between two points, and we are calculating distances for a whole image of points simulataneously, you may have to write the functions out yourself.\n",
    "* Read the documentation for some of the functions used in this lesson, and try modifying some of the inputs to change their behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
